{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"background-color:Lime;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\"> PNL_S3: Generación de texto con RNN LSTM</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La generación de texto es un tipo de problema de modelado del lenguaje.\n",
    "\n",
    "> 1. El modelado del lenguaje es el problema central para una serie de tareas de procesamiento del lenguaje natural, como voz a texto, sistemas conversacionales y el resumen de texto. \n",
    "\n",
    "> 2. Un modelo de lenguaje entrenado aprende la probabilidad de aparición de una palabra en función de la secuencia anterior de palabras utilizadas en el texto. \n",
    "\n",
    "> 3. Los modelos de lenguaje se pueden operar a nivel de carácter, nivel de n-grama, nivel de oración o incluso nivel de párrafo. \n",
    "\n",
    "**En este notebook:** se creará un modelo de lenguaje para generar texto en lenguaje natural implementando y entrenando una red neuronal recurrente del tipo LSTM.\n",
    "\n",
    "#### Conjunto de datos:\n",
    "\n",
    "> * Para este ejercicio utilizaremos el conjunto de datos [New York Times Comments and Headlines](https://www.kaggle.com/aashita/nyt-comments)el cual posee más de 9000 registros sobre noticias publicadas por este periódico entre 2017 y 2018. Entre las características de estas noticias encontramos su encabezado o headline.  \n",
    "\n",
    "> * El objetivo de esta implementación será generar encabezados de noticias a partir del entrenamiento de una RNN del tipo LSTM que nos permita crear texto a partir de palabras claves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"background-color:Lime;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\"> 1. Importar librerías</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-25T15:05:02.672887Z",
     "iopub.status.busy": "2023-04-25T15:05:02.672192Z",
     "iopub.status.idle": "2023-04-25T15:05:03.262076Z",
     "shell.execute_reply": "2023-04-25T15:05:03.260893Z",
     "shell.execute_reply.started": "2023-04-25T15:05:02.672849Z"
    },
    "papermill": {
     "duration": 7.691488,
     "end_time": "2021-01-28T14:52:02.954686",
     "exception": false,
     "start_time": "2021-01-28T14:51:55.263198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Propósito general\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import pandas as pd\n",
    "import string, os \n",
    "\n",
    "# Librerías para crear y entrenar el modelo\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import tensorflow.keras.utils as ku \n",
    "\n",
    "# Cálculo numérico y semillas aleatorias\n",
    "import numpy as np\n",
    "np.random.seed(7)\n",
    "tf.random.set_seed(7) \n",
    "import random\n",
    "random.seed(7)\n",
    "tf.random.uniform([1], seed=1)\n",
    "\n",
    "# Administrar advertencias \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012256,
     "end_time": "2021-01-28T14:52:02.980683",
     "exception": false,
     "start_time": "2021-01-28T14:52:02.968427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <p style=\"background-color:Lime;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\"> 2. Cargar el dataset</p>\n",
    "\n",
    "\n",
    "> * En esta ocasión crearemos una función que nos permita extraer del dataset sólo los datos contenidos en la columna \"headline\", la cual es la información de interés. Para ello:\n",
    "\n",
    "1. Disponemos del dataset en archivo .csv (NewYork_data.csv) en el ordenador \n",
    "2. Este dataset se encuentra en una carpeta denominada \"NewYork\" la cual está en la misma carpeta del notebook\n",
    "3. Crearemos una función que busque en la carpeta definida: \"NewYork\", el archivo .csv del dataset  \n",
    "4. La misma función extraerá del dataset los datos de la columna \"headline\" \n",
    "5. Con estos datos creará una lista de texto con la que trabajaremos en esta implementación: \"all_headlines\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Finding an Expansive View  of a Forgotten People in Niger',\n",
       " 'And Now,  the Dreaded Trump Curse',\n",
       " 'Venezuela’s Descent Into Dictatorship',\n",
       " 'Stain Permeates Basketball Blue Blood',\n",
       " 'Taking Things for Granted',\n",
       " 'The Caged Beast Awakens',\n",
       " 'An Ever-Unfolding Story',\n",
       " 'O’Reilly Thrives as Settlements Add Up',\n",
       " 'Mouse Infestation',\n",
       " 'Divide in G.O.P. Now Threatens Trump Tax Plan']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear objetos y función para cargar dataset con sólo los headlines de las noticias\n",
    "\n",
    "# Definimos objeto curr_dir para definir la ruta de acceso a la carpeta del ordenador donde está el dataset \"crudo\"\n",
    "curr_dir = './NewYork/'\n",
    "# Definimos un objeto del tipo lista \"all_headlines\" para colectar en él los encabezados de las noticias del dataset\n",
    "all_headlines = []\n",
    "\n",
    "# Creamos función en búcle para buscar el dataset, extraer la info de la columna \"headline\" y crear una lista con ellos\n",
    "\n",
    "# para cada archivo contenido en la carpeta \"NewYork\" (que está en la misma carpeta del notebook)\n",
    "for filename in os.listdir(curr_dir):\n",
    "    # si en la carpeta existe un archivo que contiene en su nombre las palabras \"NewYork\" \n",
    "    if 'NewYork' in filename:\n",
    "        # Definir objeto para leer archivo .csv que cumpla con la condición de nombre establecido \"NewYork\"\n",
    "        article_df = pd.read_csv(curr_dir + filename)\n",
    "        # colectar información contenida en la columna \"headline\" del dataset en la lista creada \"all_headlines\"\n",
    "        all_headlines.extend(list(article_df.headline.values))\n",
    "        # finalizar función (queremos que solo lo haga con el dataset que cumpla con la condición de nombre \"NewYork\")\n",
    "        break\n",
    "\n",
    "\n",
    "# mostrar dimensión del dataset creada (lista)\n",
    "print(len(all_headlines))\n",
    "\n",
    "# mostrar primeros 10 registros del dataset creado (lista)\n",
    "all_headlines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"background-color:Lime;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\"> 3. Preprocesamiento de datos</p>\n",
    "\n",
    "\n",
    "#### 3.1. Limpieza de datos\n",
    "\n",
    "* Realizaremos una limpieza de los datos de texto colectados. Esto incluye crear una función que:\n",
    "\n",
    "1. Eliminar signos de puntuación \n",
    "2. Convertir a minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T15:05:03.264483Z",
     "iopub.status.busy": "2023-04-25T15:05:03.264127Z",
     "iopub.status.idle": "2023-04-25T15:05:03.275051Z",
     "shell.execute_reply": "2023-04-25T15:05:03.274057Z",
     "shell.execute_reply.started": "2023-04-25T15:05:03.264454Z"
    },
    "papermill": {
     "duration": 0.020623,
     "end_time": "2021-01-28T14:52:03.013681",
     "exception": false,
     "start_time": "2021-01-28T14:52:02.993058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finding an expansive view  of a forgotten people in niger',\n",
       " 'and now  the dreaded trump curse',\n",
       " 'venezuelas descent into dictatorship',\n",
       " 'stain permeates basketball blue blood',\n",
       " 'taking things for granted',\n",
       " 'the caged beast awakens',\n",
       " 'an everunfolding story',\n",
       " 'oreilly thrives as settlements add up',\n",
       " 'mouse infestation',\n",
       " 'divide in gop now threatens trump tax plan']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos función para elimnar puntuaciones y convertir a minúsculas\n",
    "\n",
    "# definimos función \"clean_text\" para archivo de texto\n",
    "def clean_text(txt):\n",
    "    # objeto \"txt\" para remover signos de puntuación y convertir a minúscula \n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "    # codificar la cadena de texto (string) en formato UTF-8 y decodificarlo en formato Ascii\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return txt \n",
    "\n",
    "# aplicamos la función creada al corpus \n",
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "\n",
    "# Mostrar primeros diez registros del corpus procesado\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Tokenización: tokens tipo Ngram \n",
    "\n",
    "* El modelado del lenguaje requiere una secuencia de datos de entrada, ya que dada una secuencia (de palabras/tokens), el objetivo es predecir la siguiente palabra/token.\n",
    "\n",
    "* La tokenización es un proceso de extracción de tokens (términos/palabras) de un corpus. \n",
    "\n",
    "* La biblioteca Keras de Python tiene un modelo incorporado para la tokenización que se puede usar para obtener los tokens y su índice en el corpus. Esto dará como resultado una secuencia de tokens creados a partir de la fragmentación del corpus. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T15:05:03.276915Z",
     "iopub.status.busy": "2023-04-25T15:05:03.276601Z",
     "iopub.status.idle": "2023-04-25T15:05:03.286611Z",
     "shell.execute_reply": "2023-04-25T15:05:03.285659Z",
     "shell.execute_reply.started": "2023-04-25T15:05:03.276874Z"
    },
    "papermill": {
     "duration": 0.021253,
     "end_time": "2021-01-28T14:52:03.047418",
     "exception": false,
     "start_time": "2021-01-28T14:52:03.026165",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[392, 18],\n",
       " [392, 18, 5167],\n",
       " [392, 18, 5167, 524],\n",
       " [392, 18, 5167, 524, 4],\n",
       " [392, 18, 5167, 524, 4, 2],\n",
       " [392, 18, 5167, 524, 4, 2, 1602],\n",
       " [392, 18, 5167, 524, 4, 2, 1602, 135],\n",
       " [392, 18, 5167, 524, 4, 2, 1602, 135, 5],\n",
       " [392, 18, 5167, 524, 4, 2, 1602, 135, 5, 1952],\n",
       " [7, 58]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definimos función \"get_sequence_of_tokens\" para aplicar al corpus\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    # aplicamos la función \".fit_to_texts\" de TF para aplicar al corpus\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # almacenamos en \"total_words\" los índices de los tokens\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # Convertir los datos en una secuencia etiquetada\n",
    "    input_sequences = []\n",
    "    # para cada línea en el corpus\n",
    "    for line in corpus:\n",
    "        # objeto para aplicar función .text_to_sequences de TF a cada línea del corpus\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        # función búcle para cada token \n",
    "        for i in range(1, len(token_list)):\n",
    "            # establecer token tipo Ngram\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            # Almacenar Ngramas creados\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    # como resultado darán las secuencias de entrada (tokens) y sus índices\n",
    "    return input_sequences, total_words\n",
    "\n",
    "# aplicamos la función creada al corpus para su tokenización en Ngramas\n",
    "tokenizer = Tokenizer()\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
    "\n",
    "# mostrar secuencias creadas\n",
    "inp_sequences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. En la salida de la celda anterior vemos datos del tipo [392,18], [392,18,5176], [392,18,5176,524] etc. \n",
    "2. Estos representan las frases ngram generadas a partir de los datos de entrada, donde cada número entero corresponde al índice de una palabra en particular en el vocabulario completo presente en el texto. Por ejemplo\n",
    "\n",
    "**Encabezado:** i stand  with the shedevils  \n",
    "**Ngrams:** | Sequencia de Tokens\n",
    "\n",
    "<table>\n",
    "<tr><td>Ngram </td><td> Sequencia de Tokens</td></tr>\n",
    "<tr> <td>i stand </td><td> [392,18] </td></tr>\n",
    "<tr> <td>i stand with </td><td> [392,18,5176] </td></tr>\n",
    "<tr> <td>i stand with the </td><td> [392,18,5176,524] </td></tr>\n",
    "<tr> <td>i stand with the shedevils </td><td> [392,18,5176,524,4] </td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Completar secuencias y obtener variables: predictors y label\n",
    "\n",
    "* Ahora que hemos generado un conjunto de datos que contiene secuencias de tokens, las diferentes secuencias pueden tener diferentes longitudes. \n",
    "* Antes de comenzar a entrenar el modelo, necesitamos rellenar las secuencias y hacer que tengan la misma longitud. \n",
    "\n",
    "Para esto, \n",
    "\n",
    "1. Usaremos la función pad_sequence de Kears. \n",
    "2. Para introducir estos datos en un modelo de aprendizaje, necesitamos crear predictores y etiquetas. \n",
    "3. Crearemos una secuencia de N-gramas como predictor y la siguiente palabra del N-grama como etiqueta. Por ejemplo:\n",
    "\n",
    "\n",
    "Encabezado:  i stand with the shedevils\n",
    "\n",
    "<table>\n",
    "<tr><td>PREDICTORES</td> <td>           ETIQUETA </td></tr>\n",
    "<tr><td>i                   </td> <td>  stand</td></tr>\n",
    "<tr><td>i stand               </td> <td>  with</td></tr>\n",
    "<tr><td>i stand with      </td> <td>  the</td></tr>\n",
    "<tr><td>i stand with the </td> <td>  shedevil</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T15:05:03.290083Z",
     "iopub.status.busy": "2023-04-25T15:05:03.289604Z",
     "iopub.status.idle": "2023-04-25T15:05:03.298404Z",
     "shell.execute_reply": "2023-04-25T15:05:03.297228Z",
     "shell.execute_reply.started": "2023-04-25T15:05:03.290041Z"
    },
    "papermill": {
     "duration": 0.020781,
     "end_time": "2021-01-28T14:52:03.080904",
     "exception": false,
     "start_time": "2021-01-28T14:52:03.060123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"background-color:Lime;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\"> 4. Crear modelo</p>\n",
    "\n",
    "\n",
    "## Redes Neuronales Recurrentes LSTM\n",
    "\n",
    "![](http://www.shivambansal.com/blog/text-lstm/2.png)\n",
    "\n",
    "* A diferencia de las redes neuronales Feed-forward en las que las salidas de activación se propagan solo en una dirección, en las RNN las salidas de activación de las neuronas se propagan en ambas direcciones (de entradas a salidas y de salidas a entradas). \n",
    "\n",
    "* Esto crea bucles en la arquitectura de la red neuronal que actúa como un \"estado de memoria\" de las neuronas.Este estado permite a las neuronas recordar lo que se ha aprendido hasta el momento.\n",
    "\n",
    "* El estado de la memoria en las RNN ofrece una ventaja sobre las redes neuronales tradicionales, pero se asocia con ellas un problema llamado gradiente de fuga. \n",
    "\n",
    "* En este problema, mientras se aprende con una gran cantidad de capas, se vuelve muy difícil para la red aprender y ajustar los parámetros de las capas anteriores Para abordar este problema, se ha desarrollado un nuevo tipo de RNN llamados Modelos LSTM (Memoria a largo plazo corto).\n",
    "\n",
    "* Los LSTM tienen un estado adicional llamado \"estado de celda\" a través del cual la red realiza ajustes en el flujo de información. La ventaja de este estado es que el modelo puede recordar u olvidar las inclinaciones de manera más selectiva.  \n",
    "\n",
    "#### Vamos a diseñar un modelo LSTM:\n",
    "\n",
    "> 1. Capa de entrada: toma la secuencia de palabras como entrada\n",
    "> 2. Capa LSTM: calcula la salida usando unidades LSTM. He agregado 100 unidades en la capa, pero este número se puede ajustar más tarde.\n",
    "> 3. Capa de abandono: una capa de regularización que apaga aleatoriamente las activaciones de algunas neuronas en la capa LSTM. Ayuda a prevenir el sobreajuste. (Capa opcional)\n",
    "> 4. Capa de salida: calcula la probabilidad de la mejor palabra siguiente posible como salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T15:05:03.312451Z",
     "iopub.status.busy": "2023-04-25T15:05:03.311548Z",
     "iopub.status.idle": "2023-04-25T15:05:03.326132Z",
     "shell.execute_reply": "2023-04-25T15:05:03.325268Z",
     "shell.execute_reply.started": "2023-04-25T15:05:03.312410Z"
    },
    "papermill": {
     "duration": 0.019866,
     "end_time": "2021-01-28T14:52:03.147775",
     "exception": false,
     "start_time": "2021-01-28T14:52:03.127909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 23, 10)            112650    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               44400     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 11265)             1137765   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1294815 (4.94 MB)\n",
      "Trainable params: 1294815 (4.94 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Para crear el modelo definiremos una función \n",
    "\n",
    "# definimos función \"create_model\" max sequence y total words\n",
    "def create_model(max_sequence_len, total_words):\n",
    "    # Dimensión de la entrada\n",
    "    input_len = max_sequence_len - 1\n",
    "    # tipo del modelo \n",
    "    model = Sequential()\n",
    "    \n",
    "    # adicionamos capa tipo Embedding)\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # adicionamos capa tipo LSTM\n",
    "    model.add(LSTM(100))\n",
    "    # adicionamos capa de dropout\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # adicionamos capa Densa de salida\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    \n",
    "    # definimos parametros de .compile para el modelo\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = 'accuracy')\n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1826aa1a-cb77-4379-a69d-e9b180945dce",
    "_uuid": "f0b16b471969dbb831cb0024e303341e11b63de4",
    "papermill": {
     "duration": 0.014921,
     "end_time": "2021-01-28T14:52:03.950192",
     "exception": false,
     "start_time": "2021-01-28T14:52:03.935271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <p style=\"background-color:Lime;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\"> 5. Entrenar modelo</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "07d5cf03-d171-4993-9f8b-18446649ecb0",
    "_uuid": "156f3303b8120cc6932e6db985cbea4a7ceb08bf",
    "execution": {
     "iopub.execute_input": "2023-04-25T15:05:04.034490Z",
     "iopub.status.busy": "2023-04-25T15:05:04.033804Z",
     "iopub.status.idle": "2023-04-25T15:06:10.403624Z",
     "shell.execute_reply": "2023-04-25T15:06:10.402420Z",
     "shell.execute_reply.started": "2023-04-25T15:05:04.034452Z"
    },
    "papermill": {
     "duration": 55.578146,
     "end_time": "2021-01-28T14:52:59.543172",
     "exception": false,
     "start_time": "2021-01-28T14:52:03.965026",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1618/1618 [==============================] - 62s 37ms/step - loss: 7.8411 - accuracy: 0.0365\n",
      "Epoch 2/100\n",
      "1618/1618 [==============================] - 68s 42ms/step - loss: 7.4359 - accuracy: 0.0485\n",
      "Epoch 3/100\n",
      "1618/1618 [==============================] - 73s 45ms/step - loss: 7.2529 - accuracy: 0.0571\n",
      "Epoch 4/100\n",
      "1618/1618 [==============================] - 78s 48ms/step - loss: 7.0510 - accuracy: 0.0651\n",
      "Epoch 5/100\n",
      "1618/1618 [==============================] - 78s 48ms/step - loss: 6.8440 - accuracy: 0.0744\n",
      "Epoch 6/100\n",
      "1618/1618 [==============================] - 82s 51ms/step - loss: 6.6235 - accuracy: 0.0802\n",
      "Epoch 7/100\n",
      "1618/1618 [==============================] - 83s 51ms/step - loss: 6.3966 - accuracy: 0.0881\n",
      "Epoch 8/100\n",
      "1618/1618 [==============================] - 82s 51ms/step - loss: 6.1649 - accuracy: 0.0936\n",
      "Epoch 9/100\n",
      "1618/1618 [==============================] - 83s 52ms/step - loss: 5.9465 - accuracy: 0.1019\n",
      "Epoch 10/100\n",
      "1618/1618 [==============================] - 84s 52ms/step - loss: 5.7287 - accuracy: 0.1113\n",
      "Epoch 11/100\n",
      "1618/1618 [==============================] - 81s 50ms/step - loss: 5.5274 - accuracy: 0.1188\n",
      "Epoch 12/100\n",
      "1618/1618 [==============================] - 82s 51ms/step - loss: 5.3333 - accuracy: 0.1314\n",
      "Epoch 13/100\n",
      "1618/1618 [==============================] - 82s 51ms/step - loss: 5.1445 - accuracy: 0.1446\n",
      "Epoch 14/100\n",
      "1618/1618 [==============================] - 82s 50ms/step - loss: 4.9658 - accuracy: 0.1593\n",
      "Epoch 15/100\n",
      "1618/1618 [==============================] - 82s 51ms/step - loss: 4.7997 - accuracy: 0.1754\n",
      "Epoch 16/100\n",
      "1618/1618 [==============================] - 80s 49ms/step - loss: 4.6392 - accuracy: 0.1917\n",
      "Epoch 17/100\n",
      "1618/1618 [==============================] - 79s 49ms/step - loss: 4.4840 - accuracy: 0.2124\n",
      "Epoch 18/100\n",
      "1618/1618 [==============================] - 79s 49ms/step - loss: 4.3419 - accuracy: 0.2286\n",
      "Epoch 19/100\n",
      "1618/1618 [==============================] - 79s 49ms/step - loss: 4.2064 - accuracy: 0.2460\n",
      "Epoch 20/100\n",
      "1618/1618 [==============================] - 79s 49ms/step - loss: 4.0768 - accuracy: 0.2636\n",
      "Epoch 21/100\n",
      "1618/1618 [==============================] - 81s 50ms/step - loss: 3.9502 - accuracy: 0.2823\n",
      "Epoch 22/100\n",
      "1618/1618 [==============================] - 77s 48ms/step - loss: 3.8353 - accuracy: 0.2960\n",
      "Epoch 23/100\n",
      "1618/1618 [==============================] - 77s 48ms/step - loss: 3.7274 - accuracy: 0.3117\n",
      "Epoch 24/100\n",
      "1618/1618 [==============================] - 81s 50ms/step - loss: 3.6273 - accuracy: 0.3274\n",
      "Epoch 25/100\n",
      "1618/1618 [==============================] - 78s 48ms/step - loss: 3.5296 - accuracy: 0.3407\n",
      "Epoch 26/100\n",
      "1618/1618 [==============================] - 76s 47ms/step - loss: 3.4375 - accuracy: 0.3564\n",
      "Epoch 27/100\n",
      "1618/1618 [==============================] - 76s 47ms/step - loss: 3.3494 - accuracy: 0.3685\n",
      "Epoch 28/100\n",
      "1618/1618 [==============================] - 75s 46ms/step - loss: 3.2712 - accuracy: 0.3782\n",
      "Epoch 29/100\n",
      "1618/1618 [==============================] - 78s 48ms/step - loss: 3.1946 - accuracy: 0.3939\n",
      "Epoch 30/100\n",
      "1618/1618 [==============================] - 76s 47ms/step - loss: 3.1202 - accuracy: 0.4020\n",
      "Epoch 31/100\n",
      "1618/1618 [==============================] - 74s 46ms/step - loss: 3.0559 - accuracy: 0.4133\n",
      "Epoch 32/100\n",
      "1618/1618 [==============================] - 76s 47ms/step - loss: 2.9886 - accuracy: 0.4238\n",
      "Epoch 33/100\n",
      "1618/1618 [==============================] - 76s 47ms/step - loss: 2.9273 - accuracy: 0.4328\n",
      "Epoch 34/100\n",
      "1618/1618 [==============================] - 76s 47ms/step - loss: 2.8638 - accuracy: 0.4432\n",
      "Epoch 35/100\n",
      "1618/1618 [==============================] - 76s 47ms/step - loss: 2.8155 - accuracy: 0.4510\n",
      "Epoch 36/100\n",
      "1618/1618 [==============================] - 76s 47ms/step - loss: 2.7575 - accuracy: 0.4599\n",
      "Epoch 37/100\n",
      "1618/1618 [==============================] - 76s 47ms/step - loss: 2.7125 - accuracy: 0.4661\n",
      "Epoch 38/100\n",
      "1618/1618 [==============================] - 80s 50ms/step - loss: 2.6570 - accuracy: 0.4749\n",
      "Epoch 39/100\n",
      "1618/1618 [==============================] - 79s 49ms/step - loss: 2.6188 - accuracy: 0.4804\n",
      "Epoch 40/100\n",
      "1618/1618 [==============================] - 77s 48ms/step - loss: 2.5722 - accuracy: 0.4906\n",
      "Epoch 41/100\n",
      "1618/1618 [==============================] - 75s 47ms/step - loss: 2.5329 - accuracy: 0.4967\n",
      "Epoch 42/100\n",
      "1618/1618 [==============================] - 76s 47ms/step - loss: 2.4923 - accuracy: 0.5047\n",
      "Epoch 43/100\n",
      "1618/1618 [==============================] - 76s 47ms/step - loss: 2.4618 - accuracy: 0.5083\n",
      "Epoch 44/100\n",
      "1618/1618 [==============================] - 75s 46ms/step - loss: 2.4230 - accuracy: 0.5139\n",
      "Epoch 45/100\n",
      "1618/1618 [==============================] - 75s 46ms/step - loss: 2.3896 - accuracy: 0.5200\n",
      "Epoch 46/100\n",
      "1618/1618 [==============================] - 75s 46ms/step - loss: 2.3559 - accuracy: 0.5236\n",
      "Epoch 47/100\n",
      "1618/1618 [==============================] - 75s 46ms/step - loss: 2.3277 - accuracy: 0.5269\n",
      "Epoch 48/100\n",
      "1618/1618 [==============================] - 76s 47ms/step - loss: 2.2930 - accuracy: 0.5351\n",
      "Epoch 49/100\n",
      "1618/1618 [==============================] - 74s 46ms/step - loss: 2.2771 - accuracy: 0.5352\n",
      "Epoch 50/100\n",
      "1618/1618 [==============================] - 75s 47ms/step - loss: 2.2380 - accuracy: 0.5429\n",
      "Epoch 51/100\n",
      "1618/1618 [==============================] - 75s 46ms/step - loss: 2.2122 - accuracy: 0.5467\n",
      "Epoch 52/100\n",
      "1618/1618 [==============================] - 75s 46ms/step - loss: 2.1858 - accuracy: 0.5520\n",
      "Epoch 53/100\n",
      "1618/1618 [==============================] - 74s 46ms/step - loss: 2.1689 - accuracy: 0.5528\n",
      "Epoch 54/100\n",
      "1618/1618 [==============================] - 75s 47ms/step - loss: 2.1435 - accuracy: 0.5577\n",
      "Epoch 55/100\n",
      "1618/1618 [==============================] - 74s 46ms/step - loss: 2.1152 - accuracy: 0.5626\n",
      "Epoch 56/100\n",
      "1618/1618 [==============================] - 75s 46ms/step - loss: 2.1029 - accuracy: 0.5638\n",
      "Epoch 57/100\n",
      "1618/1618 [==============================] - 74s 46ms/step - loss: 2.0790 - accuracy: 0.5676\n",
      "Epoch 58/100\n",
      "1618/1618 [==============================] - 75s 46ms/step - loss: 2.0551 - accuracy: 0.5734\n",
      "Epoch 59/100\n",
      "1618/1618 [==============================] - 75s 46ms/step - loss: 2.0367 - accuracy: 0.5752\n",
      "Epoch 60/100\n",
      "1618/1618 [==============================] - 75s 46ms/step - loss: 2.0177 - accuracy: 0.5786\n",
      "Epoch 61/100\n",
      "1618/1618 [==============================] - 75s 46ms/step - loss: 2.0071 - accuracy: 0.5803\n",
      "Epoch 62/100\n",
      "1618/1618 [==============================] - 74s 46ms/step - loss: 1.9908 - accuracy: 0.5828\n",
      "Epoch 63/100\n",
      "1618/1618 [==============================] - 75s 46ms/step - loss: 1.9670 - accuracy: 0.5884\n",
      "Epoch 64/100\n",
      "1618/1618 [==============================] - 75s 47ms/step - loss: 1.9540 - accuracy: 0.5891\n",
      "Epoch 65/100\n",
      "1618/1618 [==============================] - 75s 46ms/step - loss: 1.9483 - accuracy: 0.5892\n",
      "Epoch 66/100\n",
      "1618/1618 [==============================] - 75s 47ms/step - loss: 1.9283 - accuracy: 0.5929\n",
      "Epoch 67/100\n",
      "1618/1618 [==============================] - 74s 46ms/step - loss: 1.9173 - accuracy: 0.5960\n",
      "Epoch 68/100\n",
      "1618/1618 [==============================] - 75s 46ms/step - loss: 1.8977 - accuracy: 0.5966\n",
      "Epoch 69/100\n",
      "1618/1618 [==============================] - 73s 45ms/step - loss: 1.8862 - accuracy: 0.6006\n",
      "Epoch 70/100\n",
      "1618/1618 [==============================] - 74s 46ms/step - loss: 1.8765 - accuracy: 0.6009\n",
      "Epoch 71/100\n",
      "1618/1618 [==============================] - 74s 46ms/step - loss: 1.8638 - accuracy: 0.6026\n",
      "Epoch 72/100\n",
      "1618/1618 [==============================] - 75s 47ms/step - loss: 1.8542 - accuracy: 0.6057\n",
      "Epoch 73/100\n",
      "1618/1618 [==============================] - 74s 45ms/step - loss: 1.8362 - accuracy: 0.6075\n",
      "Epoch 74/100\n",
      "1618/1618 [==============================] - 74s 46ms/step - loss: 1.8295 - accuracy: 0.6082\n",
      "Epoch 75/100\n",
      "1618/1618 [==============================] - 75s 46ms/step - loss: 1.8202 - accuracy: 0.6106\n",
      "Epoch 76/100\n",
      "1618/1618 [==============================] - 82s 51ms/step - loss: 1.8104 - accuracy: 0.6122\n",
      "Epoch 77/100\n",
      "1618/1618 [==============================] - 80s 50ms/step - loss: 1.8023 - accuracy: 0.6143\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1618/1618 [==============================] - 77s 48ms/step - loss: 1.7831 - accuracy: 0.6170\n",
      "Epoch 79/100\n",
      "1618/1618 [==============================] - 78s 48ms/step - loss: 1.7753 - accuracy: 0.6177\n",
      "Epoch 80/100\n",
      "1618/1618 [==============================] - 81s 50ms/step - loss: 1.7679 - accuracy: 0.6194\n",
      "Epoch 81/100\n",
      "1618/1618 [==============================] - 74s 46ms/step - loss: 1.7599 - accuracy: 0.6184\n",
      "Epoch 82/100\n",
      "1618/1618 [==============================] - 74s 46ms/step - loss: 1.7565 - accuracy: 0.6180\n",
      "Epoch 83/100\n",
      "1618/1618 [==============================] - 75s 46ms/step - loss: 1.7414 - accuracy: 0.6226\n",
      "Epoch 84/100\n",
      "1618/1618 [==============================] - 75s 46ms/step - loss: 1.7380 - accuracy: 0.6234\n",
      "Epoch 85/100\n",
      "1618/1618 [==============================] - 73s 45ms/step - loss: 1.7321 - accuracy: 0.6242\n",
      "Epoch 86/100\n",
      "1618/1618 [==============================] - 78s 49ms/step - loss: 1.7166 - accuracy: 0.6266\n",
      "Epoch 87/100\n",
      "1618/1618 [==============================] - 80s 49ms/step - loss: 1.7116 - accuracy: 0.6267\n",
      "Epoch 88/100\n",
      "1618/1618 [==============================] - 79s 49ms/step - loss: 1.7039 - accuracy: 0.6277\n",
      "Epoch 89/100\n",
      "1618/1618 [==============================] - 80s 50ms/step - loss: 1.6984 - accuracy: 0.6298\n",
      "Epoch 90/100\n",
      "1618/1618 [==============================] - 79s 49ms/step - loss: 1.6925 - accuracy: 0.6323\n",
      "Epoch 91/100\n",
      "1618/1618 [==============================] - 78s 48ms/step - loss: 1.6846 - accuracy: 0.6338\n",
      "Epoch 92/100\n",
      "1618/1618 [==============================] - 76s 47ms/step - loss: 1.6704 - accuracy: 0.6375\n",
      "Epoch 93/100\n",
      "1618/1618 [==============================] - 74s 46ms/step - loss: 1.6704 - accuracy: 0.6342\n",
      "Epoch 94/100\n",
      "1618/1618 [==============================] - 74s 46ms/step - loss: 1.6635 - accuracy: 0.6346\n",
      "Epoch 95/100\n",
      "1618/1618 [==============================] - 76s 47ms/step - loss: 1.6560 - accuracy: 0.6381\n",
      "Epoch 96/100\n",
      "1618/1618 [==============================] - 78s 48ms/step - loss: 1.6458 - accuracy: 0.6405\n",
      "Epoch 97/100\n",
      "1618/1618 [==============================] - 78s 48ms/step - loss: 1.6550 - accuracy: 0.6368\n",
      "Epoch 98/100\n",
      "1618/1618 [==============================] - 78s 48ms/step - loss: 1.6482 - accuracy: 0.6384\n",
      "Epoch 99/100\n",
      "1618/1618 [==============================] - 80s 50ms/step - loss: 1.6342 - accuracy: 0.6412\n",
      "Epoch 100/100\n",
      "1618/1618 [==============================] - 80s 49ms/step - loss: 1.6244 - accuracy: 0.6425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x158efd44760>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entrenaremos nuestro modelo con 100 épocas \n",
    "model.fit(predictors, label, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "61e99cfe-7395-4d61-8d1a-8539103d3db5",
    "_uuid": "448bf43b123060dfe4e27cb9f12889e4fe0ed2a7",
    "papermill": {
     "duration": 0.06448,
     "end_time": "2021-01-28T14:52:59.673073",
     "exception": false,
     "start_time": "2021-01-28T14:52:59.608593",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <p style=\"background-color:Lime;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\"> 6. Generar texto</p>\n",
    "\n",
    "Para generar texto:\n",
    "\n",
    "1. Crearemos una función que prediga la siguiente palabra en función de la palabra de entrada (o texto semilla). \n",
    "2. Tokenizaremos el texto semilla, completaremos la secuencia y la pasaremos al modelo entrenado para obtener las palabras predichas. \n",
    "3. Se pueden sumar varias palabras predichas para obtener la secuencia predicha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T15:12:16.308872Z",
     "iopub.status.busy": "2023-04-25T15:12:16.308164Z",
     "iopub.status.idle": "2023-04-25T15:12:16.317035Z",
     "shell.execute_reply": "2023-04-25T15:12:16.315906Z",
     "shell.execute_reply.started": "2023-04-25T15:12:16.308835Z"
    },
    "papermill": {
     "duration": 0.021432,
     "end_time": "2021-01-28T14:52:03.115037",
     "exception": false,
     "start_time": "2021-01-28T14:52:03.093605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# crear función \n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        #predicted = model.predict_classes(token_list, verbose=0)\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted = np.argmax(predicted, axis=1)\n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Ver resultados de predicción "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T15:12:23.251018Z",
     "iopub.status.busy": "2023-04-25T15:12:23.250248Z",
     "iopub.status.idle": "2023-04-25T15:12:24.810870Z",
     "shell.execute_reply": "2023-04-25T15:12:24.809744Z",
     "shell.execute_reply.started": "2023-04-25T15:12:23.250977Z"
    },
    "papermill": {
     "duration": 1.442172,
     "end_time": "2021-01-28T14:53:01.180062",
     "exception": false,
     "start_time": "2021-01-28T14:52:59.737890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States And Good Or Becomes A\n",
      "United States And Good Or Becomes A White House To Those Is\n",
      "United States And Good Or Becomes A White House To Those Is Out To A Famous News\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"united states\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"united states\", 10, model, max_sequence_len))\n",
    "print (generate_text(\"united states\", 15, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T15:12:28.767746Z",
     "iopub.status.busy": "2023-04-25T15:12:28.767322Z",
     "iopub.status.idle": "2023-04-25T15:12:29.506167Z",
     "shell.execute_reply": "2023-04-25T15:12:29.504575Z",
     "shell.execute_reply.started": "2023-04-25T15:12:28.767713Z"
    },
    "papermill": {
     "duration": 0.696685,
     "end_time": "2021-01-28T14:53:01.942955",
     "exception": false,
     "start_time": "2021-01-28T14:53:01.246270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President Trump Is In The\n",
      "President Trump Is In The Building\n",
      "President Trump Is In The Building In\n",
      "President Trump Is In The Building In America Is The\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"president trump\", 3, model, max_sequence_len))\n",
    "print (generate_text(\"president trump\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"president trump\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"president trump\", 8, model, max_sequence_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T15:12:33.389139Z",
     "iopub.status.busy": "2023-04-25T15:12:33.388790Z",
     "iopub.status.idle": "2023-04-25T15:12:34.209065Z",
     "shell.execute_reply": "2023-04-25T15:12:34.207763Z",
     "shell.execute_reply.started": "2023-04-25T15:12:33.389109Z"
    },
    "papermill": {
     "duration": 0.690836,
     "end_time": "2021-01-28T14:53:02.701967",
     "exception": false,
     "start_time": "2021-01-28T14:53:02.011131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joe Biden Still Be Access\n",
      "Joe Biden Still Be Access And\n",
      "Joe Biden Still Be Access And A\n",
      "Joe Biden Still Be Access And A Key Time On\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"joe biden\", 3, model, max_sequence_len))\n",
    "print (generate_text(\"joe biden\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"joe biden\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"joe biden\", 8, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T15:12:41.525415Z",
     "iopub.status.busy": "2023-04-25T15:12:41.524988Z",
     "iopub.status.idle": "2023-04-25T15:12:42.307032Z",
     "shell.execute_reply": "2023-04-25T15:12:42.306037Z",
     "shell.execute_reply.started": "2023-04-25T15:12:41.525374Z"
    },
    "papermill": {
     "duration": 0.703723,
     "end_time": "2021-01-28T14:53:03.473848",
     "exception": false,
     "start_time": "2021-01-28T14:53:02.770125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India And China A Changemaker Nights\n",
      "India And China A Changemaker Nights Dont\n",
      "India And China A Changemaker Nights Dont Be\n",
      "India And China A Changemaker Nights Dont Be Far To Write\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"india and china\", 3, model, max_sequence_len))\n",
    "print (generate_text(\"india and china\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"india and china\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"india and china\", 8, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T15:12:59.187070Z",
     "iopub.status.busy": "2023-04-25T15:12:59.186386Z",
     "iopub.status.idle": "2023-04-25T15:12:59.949589Z",
     "shell.execute_reply": "2023-04-25T15:12:59.948599Z",
     "shell.execute_reply.started": "2023-04-25T15:12:59.187031Z"
    },
    "papermill": {
     "duration": 0.692243,
     "end_time": "2021-01-28T14:53:04.235535",
     "exception": false,
     "start_time": "2021-01-28T14:53:03.543292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "European Union Two Blocked In\n",
      "European Union Two Blocked In Anthony\n",
      "European Union Two Blocked In Anthony To\n",
      "European Union Two Blocked In Anthony To Fall A Divorce\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"european union\", 3, model, max_sequence_len))\n",
    "print (generate_text(\"european union\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"european union\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"european union\", 8, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 76.735179,
   "end_time": "2021-01-28T14:53:06.824879",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-28T14:51:50.089700",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
